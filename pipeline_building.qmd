---
title: "Untitled"
format: html
editor_options: 
  chunk_output_type: console
---

```{r}
# load libraries
library(tidyverse)
library(janitor)
library(googledrive)
library(googlesheets4)

# source functions
source("index_data_processing_functions.r")

data <- readRDS("logger_data.rds")
files <- readRDS("processed_files.rds")

# authenticate google drive
drive_auth()
```

https://drive.google.com/drive/u/0/folders/1MZxIPozLI_K6ietpjLzEM-ybJjGT-a48

```{r}
# Test function to get google files from drive
temp_files <- get_logger_csvs_by_id("1MZxIPozLI_K6ietpjLzEM-ybJjGT-a48")

# YES
```

```{r}
# Test fuction to get google files for ALL loggers
logger_files <- get_all_logger_csvs_by_id()

# Gorg. 


# Grab and clean metadata
metadata <- read_and_clean_metadata()
```


```{r}
source("index_data_processing_functions.r")
```


```{r}
# Test cleaning function for each sensor type ----

# PAR
par_files <- logger_files |>
  filter(sensor_type == "PAR") |>
  distinct()

df_par <- purrr::map_dfr(seq_len(nrow(par_files)), function(i) {
  read_and_clean_logger_csv(par_files[i, ], metadata)
})

unique(df_par$site)
range(df_par$datetime)

# Edmonds PAR 15605 is actually PAR 3 bottom... ... But naming convention is different and throwing of the position assignment
# Also jeffersonhead PAR6424 is PAR5 bottom


df_par_na_pos <- df_par |>
  filter(is.na(position))
# Loggers to look into where position couldn't parse:
unique(df_par_na_pos[,c('site','par_logger_id','position')])

# WL - kpa
df_wl <- read_and_clean_logger_csv(logger_files[29, ])
View(df_wl)

# WL - psi
df_wl2 <- read_and_clean_logger_csv(logger_files[32, ])
View(df_wl2)

# WL - both
wl_files <- logger_files |>
  filter(sensor_type == "WL") |>
  distinct()

df_wl <- purrr::map_dfr(seq_len(nrow(wl_files)), function(i) {
  read_and_clean_logger_csv(wl_files[i, ], metadata)
})

unique(df_wl$site)
# are vashon and pointvashon the same site?
range(df_wl$datetime)

df_wl_na_pos <- df_wl |>
  filter(is.na(position))
# Loggers to look into where position couldn't parse:
unique(df_wl_na_pos[,c('site','wl_logger_id','position')])


# TEMP
df_temp <- read_and_clean_logger_csv(logger_files[296, ], metadata = metadata)

temp_files <- logger_files |>
  filter(sensor_type == "TEMP") |>
  distinct()

df_temp <- purrr::map_dfr(seq_len(nrow(temp_files)), function(i) {
  read_and_clean_logger_csv(temp_files[i, ], metadata)
})

unique(df_temp$site)
range(df_temp$datetime)

df_temp_na_pos <- df_temp |>
  filter(is.na(position))
# Loggers to look into where position couldn't parse:
unique(df_temp_na_pos[,c('site','temp_logger_id','position')])

library(plotly)

ggplotly(df_temp |>
           # filter(site == "jeffersonhead",
           #        position == "surface",
           #        year(datetime) == 2023) |>
           ggplot() +
           geom_point(aes(x = datetime, y = temp_c, color = temp_logger_id), size = 0.1) +
           facet_wrap(~site))

# ok so... the filtering is not perfect. because some files are kinda weirdly duplicating each other 
# but then are not mentioned in the metadata.. EX: Edmonds_T3_2024-04-03 is from 12/06/2023 12:45:00 to 06/18/2024 7:45:00
# while Edmonds_T3_2023-12-14.csv is from 12/06/2023 12:45:00 to 04/03/2024 10:15:00. 
# somehow in the metadata we will want to filter Edmonds_T3_2024-04-03 to actually start at 04/03/2023


# PH
df_ph <- read_and_clean_logger_csv(logger_files[107, ])

ph_files <- logger_files |>
  filter(sensor_type == "pH") |>
  distinct()

df_ph <- purrr::map_dfr(seq_len(nrow(ph_files)), function(i) {
  read_and_clean_logger_csv(ph_files[i, ], metadata)
})

unique(df_ph$site)
range(df_ph$datetime)

df_ph_na_pos <- df_ph |>
  filter(is.na(position))
# Loggers to look into where position couldn't parse:
unique(df_ph_na_pos[,c('site','ph_logger_id','position')])

# DO
df_do <- read_and_clean_logger_csv(do_files)
View(df_do)

do_files <- logger_files |>
  filter(sensor_type == "DO") |>
  distinct()

df_do <- purrr::map_dfr(seq_len(nrow(do_files)), function(i) {
  read_and_clean_logger_csv(do_files[i, ], metadata)
})

unique(df_do$site)
range(df_do$datetime)

df_do_na_pos <- df_do |>
  filter(is.na(position))
# Loggers to look into where position couldn't parse:
unique(df_do_na_pos[,c('site','do_logger_id','position')])

# CON
con_files <- logger_files |>
  filter(sensor_type == "CON") |>
  distinct()

df_con <- purrr::map_dfr(seq_len(nrow(con_files)), function(i) {
  read_and_clean_logger_csv(con_files[i, ], metadata)
})

# What does the star mean on some site names?
# Are vashon and point vashon the same?

unique(df_con$site)
range(df_con$datetime)

df_con_na_pos <- df_con |>
  filter(is.na(position))
# Loggers to look into where position couldn't parse:
unique(df_con_na_pos[,c('site','con_logger_id','position')])
```

```{r}
all <- df_temp |>
  full_join(df_ph, by = c("site", "datetime", "position")) |>
  full_join(df_do, by = c("site", "datetime", "position")) |>
  full_join(df_wl, by = c("site", "datetime", "position")) |>
  full_join(df_par, by = c("site", "datetime", "position")) |>
  full_join(df_con, by = c("site", "datetime", "position"))
```

This is not giving what I want because... Some of these are at the surface and some are at the bottom. so im seeing what SEEM like duplicates but... Yeah. So I need to distinguish there before I joing everything. 

I need to clean the metadata as a part of all of this. Alrighty!!!!!!!!!!!

```{r}

metadata_raw <- read_sheet("https://docs.google.com/spreadsheets/d/1JJ4Vtb_pI9FJRvMYg7pbhQzM6JPcFzGVU3rR9IdYnGQ/edit?gid=1334564098#gid=1334564098",
                           sheet = " Deployments_IndexOnly",
                           na = c("", "n/a", "#N/A"),
                           col_types = "c")

metadata <- metadata_raw |>
  clean_names() |>
  rename("site" = "x1",
         "logger_id" = "nickname") |>
  separate(relaunch_file_name_recovery_file_name,
           into = c("relaunch_file_name", "recovery_file_name"),
           sep = ",")

metadata <- metadata |>
  mutate(position = tolower(position),
         logger_id = if_else(is.na(logger_id), 
                             str_split(relaunch_file_name, "_", simplify = TRUE)[, 2],
                             logger_id)) |>
  filter(!(str_detect(site, "^20\\d{2}$"))) |>
  mutate(site = tolower(site),
         site = str_remove_all(site, "[^a-zA-Z0-9]")) |>
  # dates
  mutate(launch_date_office = parse_date_time(launch_date_office,
                                              orders = c("ymd", "mdy")),
         initial_deployment_date = parse_date_time(initial_deployment_date,
                                                   orders = c("ymd", "mdy")),
         initial_deployment_time = str_remove(initial_deployment_time, "^~\\s*"),
         initial_deployment_time = parse_date_time(initial_deployment_time,
                                                   orders = c("HM", "I:M p")),
         # Combine deployment date and time
         initial_deployment_datetime = case_when(
           # if no date, datetime = NA
           is.na(initial_deployment_date) ~ as.POSIXct(NA),
           # if no time, use the date and fill 00:00:00 for time
           is.na(initial_deployment_time) ~ ymd_hms(paste(as.Date(initial_deployment_date),
                                                          "00:00:00")),
           # otherwise, combine the two 
           TRUE ~ ymd_hms(paste(as.Date(initial_deployment_date), 
                                format(initial_deployment_time, "%H:%M:%S"))))) |>
  relocate(initial_deployment_datetime, .after = initial_deployment_time)

metadata <- metadata |>
  mutate(relaunch_date = if_else(site == "edmonds" &
                                   logger_id == "DO18" &
                                   relaunch_date == "2025-04-02",
                                 "2025-03-13",
                                 relaunch_date),
         relaunch_date = parse_date_time(relaunch_date,
                                         orders = c("ymd", "mdy")))

metadata <- metadata |>
  mutate(relaunch_recovery_time = if_else(site == "edmonds" &
                                            logger_id == "DO18" &
                                            relaunch_date == "2025-04-02",
                                          "9:30",
                                          relaunch_recovery_time),
         relaunch_recovery_time = parse_date_time(relaunch_recovery_time,
                                                  orders = c("HM", "I:M p")))

metadata <- metadata |>
  mutate( 
    # Combine relauch date and time
    relaunch_recovery_datetime = case_when(
      # if no date, datetime = NA
      is.na(relaunch_date) ~ as.POSIXct(NA),
      # if no time, use the date and fill 00:00:00 for time
      is.na(relaunch_recovery_time) ~ ymd_hms(paste(as.Date(relaunch_date),
                                                    "00:00:00")),
      # otherwise, combine the two 
      TRUE ~ ymd_hms(paste(as.Date(relaunch_date), 
                           format(relaunch_recovery_time, "%H:%M:%S")))), 
    .after = relaunch_recovery_time)

metadata <- metadata |>
  mutate(
    # Relauch Deployment Datetime
    relaunch_deployment_time = str_remove(relaunch_deployment_time, "^~\\s*"),
    relaunch_deployment_time = parse_date_time(relaunch_deployment_time,
                                               orders = c("HM", "I:M p", "I:M:S p")))

metadata <- metadata |>
  mutate(
    relaunch_deployment_datetime = case_when(
      # if no date, datetime = NA
      is.na(relaunch_date) ~ as.POSIXct(NA),
      # if no time, use the date and fill 00:00:00 for time
      is.na(relaunch_deployment_time) ~ ymd_hms(paste(as.Date(relaunch_date),
                                                      "00:00:00")),
      # otherwise, combine the two 
      TRUE ~ ymd_hms(paste(as.Date(relaunch_date), 
                           format(relaunch_deployment_time, "%H:%M:%S")))), 
    .after = relaunch_deployment_time)

metadata <- metadata |>
  mutate(relaunch_data_readout_date = parse_date_time(relaunch_data_readout_date,
                                                      orders = c("ymd", "mdy")))

metadata <- metadata |>
  mutate(recovery_date = parse_date_time(recovery_date,
                                         orders = c("ymd", "mdy")),
         recovery_time = str_remove(recovery_time, "^~\\s*"),
         recovery_time = parse_date_time(recovery_time,
                                         orders = c("HM", "I:M p", "I:M:S p")),
         data_readout_date = parse_date_time(data_readout_date,
                                             orders = c("ymd", "mdy")))

metadata <- metadata |>
  separate(file_upload_date,
           into = c("file_upload_date_1", "file_upload_date_2"),
           sep = ",") |>
  mutate(file_upload_date_1 = parse_date_time(file_upload_date_1,
                                              orders = c("ymd", "mdy")),
         file_upload_date_2 = parse_date_time(file_upload_date_2,
                                              orders = c("ymd", "mdy")))
```

Assumptions:

the relaunch date on this row is incorrect and should actually be 2025-03-13:
Edmonds	BOTTOM	DO18	21556613	12/10/2024	2024-12-11	11:08	2025-04-02	Shuttle	9:30:00 AM @ 3-13	11:45:00 AM	2025-03-14				


Streamline that metadata cleaning into a function:

```{r}
read_and_clean_metadata <- function(metadata_file_url = "https://docs.google.com/spreadsheets/d/1JJ4Vtb_pI9FJRvMYg7pbhQzM6JPcFzGVU3rR9IdYnGQ/edit?gid=1334564098#gid=1334564098",
                                    sheet_name = " Deployments_IndexOnly"){
  
  # Read in metadata sheet from google drive
  metadata_raw <- read_sheet(metadata_file_url,
                             sheet = sheet_name,
                             na = c("", "n/a", "#N/A"),
                             col_types = "c")
  
  # Cleaning
  metadata <- metadata_raw |>
    clean_names() |>
    # Rename nickname to logger_id to match logger file convention
    rename("site" = "x1",
           "logger_id" = "nickname") |>
    separate(relaunch_file_name_recovery_file_name,
             into = c("relaunch_file_name", "recovery_file_name"),
             sep = ",") |>
    # Make position consistently lowercase
    mutate(position = tolower(position),
           # If no logger_id in column, pull from filename
           logger_id = if_else(is.na(logger_id), 
                               str_split(relaunch_file_name, "_", simplify = TRUE)[, 2],
                               logger_id)) |>
    # Remove rows that are year seperators
    filter(!(str_detect(site, "^20\\d{2}$"))) |>
    # Make site name consistently lowercase with no spaces or puncuation
    mutate(site = tolower(site),
           site = str_remove_all(site, "[^a-zA-Z0-9]")) |>
    # Dates
    mutate(launch_date_office = parse_date_time(launch_date_office,
                                                orders = c("ymd", "mdy")),
           initial_deployment_date = parse_date_time(initial_deployment_date,
                                                     orders = c("ymd", "mdy")),
           initial_deployment_time = str_remove(initial_deployment_time, "^~\\s*"),
           initial_deployment_time = parse_date_time(initial_deployment_time,
                                                     orders = c("HM", "I:M p")),
           # Combine deployment date and time
           initial_deployment_datetime = case_when(
             # if no date, datetime = NA
             is.na(initial_deployment_date) ~ as.POSIXct(NA),
             # if no time, use the date and fill 00:00:00 for time
             is.na(initial_deployment_time) ~ ymd_hms(paste(as.Date(initial_deployment_date),
                                                            "00:00:00")),
             # otherwise, combine the two 
             TRUE ~ ymd_hms(paste(as.Date(initial_deployment_date), 
                                  format(initial_deployment_time, "%H:%M:%S"))))) |>
    relocate(initial_deployment_datetime, .after = initial_deployment_time) |>
    mutate(relaunch_date = if_else(site == "edmonds" &
                                     logger_id == "DO18" &
                                     relaunch_date == "2025-04-02",
                                   "2025-03-13",
                                   relaunch_date),
           relaunch_date = parse_date_time(relaunch_date,
                                           orders = c("ymd", "mdy"))) |>
    mutate(relaunch_recovery_time = if_else(site == "edmonds" &
                                              logger_id == "DO18" &
                                              relaunch_date == "2025-04-02",
                                            "9:30",
                                            relaunch_recovery_time),
           relaunch_recovery_time = parse_date_time(relaunch_recovery_time,
                                                    orders = c("HM", "I:M p"))) |>
    mutate( 
      # Combine relauch date and time
      relaunch_recovery_datetime = case_when(
        # if no date, datetime = NA
        is.na(relaunch_date) ~ as.POSIXct(NA),
        # if no time, use the date and fill 00:00:00 for time
        is.na(relaunch_recovery_time) ~ ymd_hms(paste(as.Date(relaunch_date),
                                                      "00:00:00")),
        # otherwise, combine the two 
        TRUE ~ ymd_hms(paste(as.Date(relaunch_date), 
                             format(relaunch_recovery_time, "%H:%M:%S")))), 
      .after = relaunch_recovery_time) |>
    mutate(
      # Relauch Deployment Datetime
      relaunch_deployment_time = str_remove(relaunch_deployment_time, "^~\\s*"),
      relaunch_deployment_time = parse_date_time(relaunch_deployment_time,
                                                 orders = c("HM", "I:M p", "I:M:S p"))) |>
    mutate(
      relaunch_deployment_datetime = case_when(
        # if no date, datetime = NA
        is.na(relaunch_date) ~ as.POSIXct(NA),
        # if no time, use the date and fill 00:00:00 for time
        is.na(relaunch_deployment_time) ~ ymd_hms(paste(as.Date(relaunch_date),
                                                        "00:00:00")),
        # otherwise, combine the two 
        TRUE ~ ymd_hms(paste(as.Date(relaunch_date), 
                             format(relaunch_deployment_time, "%H:%M:%S")))), 
      .after = relaunch_deployment_time) |>
    mutate(relaunch_data_readout_date = parse_date_time(relaunch_data_readout_date,
                                                        orders = c("ymd", "mdy"))) |>
    mutate(recovery_date = parse_date_time(recovery_date,
                                           orders = c("ymd", "mdy")),
           recovery_time = str_remove(recovery_time, "^~\\s*"),
           recovery_time = parse_date_time(recovery_time,
                                           orders = c("HM", "I:M p", "I:M:S p")),
           data_readout_date = parse_date_time(data_readout_date,
                                               orders = c("ymd", "mdy"))) |>
    separate(file_upload_date,
             into = c("file_upload_date_1", "file_upload_date_2"),
             sep = ",") |>
    mutate(file_upload_date_1 = parse_date_time(file_upload_date_1,
                                                orders = c("ymd", "mdy")),
           file_upload_date_2 = parse_date_time(file_upload_date_2,
                                                orders = c("ymd", "mdy")))
  
  return(metadata)
  
}

```


Test out metadata cleaning function
```{r}
metadata <- read_and_clean_metadata()



position <- metadata$position[metadata$site == "jeffersonhead" &
                                metadata$logger_id == "T2" &
                                metadata$initial_deployment_date == as.Date("2023-01-20")]

unique(metadata$site)
unique(all$site)
```


```{r}
# attempt to filter out stuff from metadata... ... ... 
# start with just Jefferson head temp for simplicity
jh <- all |>
  filter(site == "jeffersonhead") |>
  select(c("site", "position", "temp_logger_id", "temp_c", "datetime")) |>
  # add in missing times as NA to complete timeseries
  group_by(site, position) |>
  complete(datetime = seq(min(datetime), max(datetime), by = "15 mins")) |>
  ungroup() |>
  distinct() #|>
#  filter(position == "surface")

# I want to filter out datetime that:
## starts before initial_deployment_datetime in metadata, 
## is between relaunch_recovery_datetime and relaunch_deployment_datetime
## is after recovery_datetime

initial_deployment_date <- metadata$initial_deployment_datetime[
  metadata$site == "edmonds" &
    metadata$logger_id == "T11" &
    metadata$initial_deployment_date == as.Date("2024-06-13")
]
relaunch_recovery_date <- metadata$relaunch_recovery_datetime[
  metadata$site == "edmonds" &
    metadata$logger_id == "T11" &
    metadata$initial_deployment_date == as.Date("2024-06-13")
]
relaunch_deployment_date <- metadata$relaunch_deployment_datetime[
  metadata$site == "edmonds" &
    metadata$logger_id == "T11" &
    metadata$initial_deployment_date == as.Date("2024-06-13")
]
recovery_date <- metadata$recovery_datetime[
  metadata$site == "edmonds" &
    metadata$logger_id == "T11" &
    metadata$initial_deployment_date == as.Date("2024-06-13")
]


test_outofwater_filter <- df_temp |>
  mutate(
    temp_c = case_when(
      datetime <= initial_deployment_date ~ NA_real_,
      datetime >= relaunch_recovery_date & datetime <= relaunch_deployment_date ~ NA_real_,
      datetime >= recovery_date ~ NA_real_,
      TRUE ~ temp_c
    )
  )



ggplot(df_temp) +
  geom_line(aes(x = datetime, y = temp_c, color = position))

ggplot(test_outofwater_filter) +
  geom_line(aes(x = datetime, y = temp_c, color = position))
```

Ok.... let's try the big guy


```{r}
range(data$datetime)
unique(data$site)

# surface temp visual check
logger_data |>
  filter(position == "surface") |>
  ggplot() +
  geom_point(aes(x = datetime, y = tidbit_temp_c, color = temp_logger_id), size = 0.1) +
  facet_wrap(~site)

# surface ph
logger_data |>
  filter(position == "surface") |>
  filter(pH <= 10) |>
  ggplot() +
  geom_point(aes(x = datetime, y = pH, color = ph_logger_id), size = 0.1) +
  facet_wrap(~site) +
  labs(title = "pH at Surface by Site",
       x = "Date",
       y = "pH") +
  theme_minimal()

# bottom ph
logger_data |>
  filter(position == "bottom") |>
  ggplot() +
  geom_point(aes(x = datetime, y = pH, color = ph_logger_id), size = 0.1) +
  facet_wrap(~site) +
  labs(title = "pH at Bottom by Site",
       x = "Date",
       y = "pH") +
  theme_minimal()


# surface do
logger_data |>
  filter(position == "surface") |>
  filter(do_conc_mg_per_L >= 0) |>
  ggplot() +
  geom_point(aes(x = datetime, y = do_conc_mg_per_L, color = do_logger_id), size = 0.1) +
  facet_wrap(~site) +
  labs(title = "Dissolved Oxygen at Surface by Site",
       x = "Date",
       y = "Dissolved Oxygen (mg/L)") +
  theme_minimal()

# bottom do
logger_data |>
  filter(position == "bottom") |>
  filter(do_conc_mg_per_L >= 0) |>
  ggplot() +
  geom_point(aes(x = datetime, y = do_conc_mg_per_L, color = do_logger_id), size = 0.1) +
  facet_wrap(~site) +
  labs(title = "Dissolved Oxygen at Bottom by Site",
       x = "Date",
       y = "Dissolved Oxygen (mg/L)") +
  theme_minimal()


# bottom wl
logger_data |>
  filter(position == "bottom") |>
  ggplot() +
  geom_point(aes(x = datetime, y = abs_pres_kpa, color = wl_logger_id), size = 0.1) +
  facet_wrap(~site) +
  labs(title = "Water Level at Bottom by Site",
       x = "Date",
       y = "Absolute Pressure (kpa)") +
  theme_minimal()


# bottom par (raw)
logger_data |>
  filter(position == "bottom") |>
  ggplot() +
  geom_point(aes(x = datetime, y = raw_integrating_light, color = par_logger_id), size = 0.1) +
  facet_wrap(~site) +
  labs(title = "PAR Raw at Bottom by Site",
       x = "Date",
       y = "Raw Integrating Light") +
  theme_minimal()

# bottom par (calibrated)
logger_data |>
  filter(position == "bottom") |>
  ggplot() +
  geom_point(aes(x = datetime, y = calibrated_integrating_light, color = par_logger_id), size = 0.1) +
  facet_wrap(~site) +
  labs(title = "PAR Calibrated at Bottom by Site",
       x = "Date",
       y = "Calibrated Integrating Light") +
  theme_minimal()
# urm calibrated is the same as raw?

# surface con
logger_data |>
  filter(position == "surface") |>
  ggplot() +
  geom_point(aes(x = datetime, y = high_range_microsiemens_per_cm, color = con_logger_id), size = 0.1) +
  facet_wrap(~site) +
  labs(title = "Conductivity at Surface by Site",
       x = "Date",
       y = "Conductivity (µS/cm)") +
  theme_minimal()

# bottom con
logger_data |>
  filter(position == "bottom") |>
  ggplot() +
  geom_point(aes(x = datetime, y = high_range_microsiemens_per_cm, color = con_logger_id), size = 0.1) +
  facet_wrap(~site) +
  labs(title = "Conductivity at Bottom by Site",
       x = "Date",
       y = "Conductivity (µS/cm)") +
  theme_minimal()

# comparing all temp measures
logger_data |>
  select(site, position, datetime, tidbit_temp_c, ph_temp_c, do_temp_c, wl_temp_c, con_temp_c) |>
  pivot_longer(
    cols = c(tidbit_temp_c, ph_temp_c, do_temp_c, wl_temp_c, con_temp_c),
    names_to = "temp_sensor_type",
    values_to = "temperature_c"
  ) |>
  filter(position == "surface",
         temperature_c >= 0,
         temperature_c <= 25) |>
  ggplot(aes(x = datetime, y = temperature_c, color = temp_sensor_type)) +
  geom_line(alpha = 0.7) +
  facet_wrap(~site) +
  labs(
    title = "Temperature Measurements at Surface by Site",
    x = "Date/Time", 
    y = "Temperature (°C)",
    color = "Sensor Type"
  ) +
  theme_minimal()

logger_data |>
  select(site, position, datetime, tidbit_temp_c, ph_temp_c, do_temp_c, wl_temp_c, con_temp_c) |>
  pivot_longer(
    cols = c(tidbit_temp_c, ph_temp_c, do_temp_c, wl_temp_c, con_temp_c),
    names_to = "temp_sensor_type",
    values_to = "temperature_c"
  ) |>
  filter(position == "bottom",
         temperature_c >= 0,
         temperature_c <= 25) |>
  ggplot(aes(x = datetime, y = temperature_c, color = temp_sensor_type)) +
  geom_line(alpha = 0.7) +
  facet_wrap(~site) +
  labs(
    title = "Temperature Measurements at Bottom by Site",
    x = "Date/Time", 
    y = "Temperature (°C)",
    color = "Sensor Type"
  ) +
  theme_minimal()

```


So glad that this is seemingly working so far! Next steps for me:
- Find a way to validate this against some raw files.
- Work on more ways to filter outliers
- Would love to be able to update and grab new files without needing to re-process all of the old ones as well (GOT IT!!)
- Bring in temp from other instruments as well (do we want to include all?)



```{r}
# save what I have for now - may be useful in data meeting tomorrow
# write_rds(please_lord, "all_loggers.rds")
```


```{r}
# Lets try the incremental function
data <- update_logger_data_incremental()
```


---

Manually check a few files that we suspect are duplicates in the drive:
```{r}
ed_ph_1 <- read_csv(here::here("data", "Edmonds_pH5_2023-01-20.csv"))
ed_ph_2 <- read_csv(here::here("data", "Edmonds_pH5_2023-01-20-2.csv"))

library(dataCompareR)

comparison_result <- rCompare(ed_ph_1, ed_ph_2)
summary(comparison_result)

ed_wl_1 <- read_csv(here::here("data", "Edmonds_WL6_2024-09-04.csv"))
ed_wl_2 <- read_csv(here::here("data", "Edmonds_WL6_2024-09-04 copy.csv"))

comparison_result <- rCompare(ed_wl_1, ed_wl_2)
summary(comparison_result)

jh_wl_1 <- read_csv(here::here("data", "JeffersonHead_WL5_2024-09-04.csv"))
jh_wl_2 <- read_csv(here::here("data", "JeffersonHead_WL5_2024-09-04 copy.csv"))

comparison_result <- rCompare(jh_wl_1, jh_wl_2)
summary(comparison_result)

wp_wl_1 <- read_csv(here::here("data", "WingPoint_WL4_2024-09-04.csv"))
wp_wl_2 <- read_csv(here::here("data", "WingPoint_WL4_2024-09-04 copy.csv"))

comparison_result <- rCompare(wp_wl_1, wp_wl_2)
summary(comparison_result)
```


---

```{r}
# Check if you have multiple rows for ANY site/position/datetime combination
duplicates_check <- logger_data |>
  group_by(site, position, datetime) |>
  filter(n() > 1) |>
  ungroup()

nrow(duplicates_check)  # Should tell you how many duplicate rows exist

# If there are duplicates, look at a sample
if(nrow(duplicates_check) > 0) {
  duplicates_check |>
    select(site, position, datetime) |>
    distinct() |>
    head(10)
}
```

```{r}
write_csv(logger_data, here("data", "logger_data.csv"))

```

```{r}
df <- data |>
  filter(position == "other")

unique(df$site)

df <- df |>
  filter(site == "jeffersonhead")


df <- data |>
  filter(position == "foulweatherbluff",
         do_logger_id == "DO16")

"NorthwestNarrows_DO8_2025-02-18.csv" %in% processed_files


```


```{r}
"Edmonds_DNR6428_2023-10-13.CSV" %in% processed_files
"FoulweatherBluff_CON32_2025-02-18.csv" %in% processed_files
"NorthwestNarrows_DO7_2025-05-21.csv" %in% processed_files
```

```{r}
# Load your saved data
data <- readRDS(here::here("data", "logger_data.rds"))

# Check for records from this specific file
data |> 
  filter(site == "edmonds", 
         wl_logger_id == "DNR6428",  # or appropriate logger_id column
         datetime >= as.Date("2023-10-13")) |> 
  nrow()

processed <- readRDS(here::here("data", "processed_files.rds"))
processed <- processed[!processed %in% c("NorthwestNarrows_DO7_2025-05-21.csv")]
saveRDS(processed, here::here("data", "processed_files.rds"))
```

```{r}
# Check total DO7 rows
data |> 
  filter(do_logger_id == "DO7") |> 
  nrow()

# Check date range of DO7
data |> 
  filter(do_logger_id == "DO7") |> 
  summarise(min_date = min(datetime, na.rm = TRUE), 
            max_date = max(datetime, na.rm = TRUE))

# Check if there are NAs in the May-Aug range
data |> 
  filter(do_logger_id == "DO7") |>
  filter(datetime >= as.POSIXct("2025-05-21")) |>
  summarise(
    total_rows = n(),
    na_do_conc = sum(is.na(do_conc_mg_per_L)),
    non_na_do_conc = sum(!is.na(do_conc_mg_per_L))
  )
```

